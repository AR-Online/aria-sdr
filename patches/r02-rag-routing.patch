diff --git a/main.py b/main.py
index 27af6fd..0bcd3aa 100644
--- a/main.py
+++ b/main.py
@@ -1,103 +1,140 @@
-import os
-import re
-import time
-import secrets
-from dotenv import load_dotenv, find_dotenv
-load_dotenv(find_dotenv(), override=True)
-from typing import Any, Dict, Optional, Tuple, List
-from fastapi import FastAPI, HTTPException, Depends, status
+# main.py
+from __future__ import annotations
+
+import os, re, time, secrets, json, logging
+from typing import Any, Dict, List, Optional, Tuple
+
+import requests
+from requests.adapters import HTTPAdapter
+from urllib3.util.retry import Retry
+from fastapi import FastAPI, Depends, HTTPException, Request, status
 from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
 from pydantic import BaseModel
 from dotenv import load_dotenv, find_dotenv

-# ---------------------------------------------------------------------------
-# Config / Credenciais
-# ---------------------------------------------------------------------------
+# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
+# Boot / Config
+# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
 load_dotenv(find_dotenv(), override=True)
+app = FastAPI(title="ARIA Endpoint")

-OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
-ASSISTANT_ID   = os.getenv("ASSISTANT_ID")
-AUTH_TOKEN     = os.getenv("AUTH_TOKEN", "dev-token")
-ASSISTANT_TIMEOUT_SECONDS = float(os.getenv("ASSISTANT_TIMEOUT_SECONDS", "12"))
+API_TOKEN = (os.getenv("FASTAPI_BEARER_TOKEN") or "").strip()
+auth_scheme = HTTPBearer(auto_error=False)
+log = logging.getLogger(__name__)

-try:
-    from openai import OpenAI
-    client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None
-except Exception:
-    client = None
-
-app = FastAPI(title="ARIA Webhook")
-security = HTTPBearer()
-
-# ---------------------------------------------------------------------------
-# Auth
-# ---------------------------------------------------------------------------
-
-def require_auth(credentials: HTTPAuthorizationCredentials = Depends(security)) -> str:
-    token = credentials.credentials or ""
-    if token != AUTH_TOKEN:
-        raise HTTPException(
-            status_code=status.HTTP_401_UNAUTHORIZED,
-            detail="Token inv??lido",
-            headers={"WWW-Authenticate": "Bearer"},
+# Session para RAG com retry/backoff
+_rag_session: Optional[requests.Session] = None
+
+def get_rag_session() -> requests.Session:
+    global _rag_session
+    if _rag_session is None:
+        s = requests.Session()
+        retries = Retry(
+            total=2,
+            backoff_factor=0.3,
+            status_forcelist=[429, 500, 502, 503, 504],
+            allowed_methods=["POST"],
         )
-    return token
+        adapter = HTTPAdapter(max_retries=retries)
+        s.mount("http://", adapter)
+        s.mount("https://", adapter)
+        _rag_session = s
+    return _rag_session

-# ---------------------------------------------------------------------------
-# Modelos
-# ---------------------------------------------------------------------------
+def require_auth(cred: HTTPAuthorizationCredentials = Depends(auth_scheme)) -> str:
+    if not cred:
+        raise HTTPException(401, "Missing Authorization header")
+    token = (cred.credentials or "").strip()
+    if token != API_TOKEN:
+        raise HTTPException(401, "Token inv??lido")
+    return token

+# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
+# Models (uma vez s??)
+# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
 class AssistRequest(BaseModel):
-    # Aceita 'input' OU 'user_text' para compatibilidade
     input: Optional[str] = None
     user_text: Optional[str] = None
-    thread_id: Optional[str] = None
     variables: Optional[Dict[str, Any]] = None
+    channel: Optional[str] = None
+    thread_id: Optional[str] = None
+    metadata: Optional[Dict[str, Any]] = None

 class AssistResponse(BaseModel):
     reply_text: str
     route: Optional[str] = None
+    thread_id: Optional[str] = None
     variables: Optional[Dict[str, Any]] = None
     confidence: Optional[float] = None
     next_action: Optional[str] = None
     tags: Optional[List[str]] = None
-    thread_id: str

-# ---------------------------------------------------------------------------
-# Regras de neg??cio: triagem + volumetria
-# ---------------------------------------------------------------------------
+# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
+# OpenAI (Assistants) ??? opcional
+# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
+OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
+ASSISTANT_ID = os.getenv("ASSISTANT_ID")
+ASSISTANT_TIMEOUT_SECONDS = float(os.getenv("ASSISTANT_TIMEOUT_SECONDS", "12"))
+
+try:
+    from openai import OpenAI  # type: ignore
+    client_assistant = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None
+except Exception:  # pragma: no cover
+    OpenAI = None  # type: ignore
+    client_assistant = None
+
+def wait_run(thread_id: str, run_id: str, timeout_seconds: Optional[float] = None):
+    if client_assistant is None:
+        return None
+    deadline = time.time() + timeout_seconds if timeout_seconds and timeout_seconds > 0 else None
+    while True:
+        run = client_assistant.beta.threads.runs.retrieve(thread_id=thread_id, run_id=run_id)
+        if run.status in ("completed", "failed", "cancelled", "expired", "requires_action"):
+            return run
+        if deadline and time.time() >= deadline:
+            return None
+        time.sleep(0.5)
+
+def last_assistant_message(thread_id: str) -> str:
+    if client_assistant is None:
+        return ""
+    msgs = client_assistant.beta.threads.messages.list(thread_id=thread_id)
+    for m in msgs.data:
+        if getattr(m, "role", "") == "assistant":
+            try:
+                return m.content[0].text.value
+            except Exception:
+                continue
+    return ""

+# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
+# Regras de neg??cio ??? triagem + volumetria
+# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
 def classify_route(user_text: str, v: Dict[str, Any]) -> Tuple[Optional[str], Dict[str, str], Optional[str]]:
-    """Decide rota (envio/recebimento) e, se envio, classifica volumetria.
-    Retorna: (route, variables_out, next_action)
-    """
     t = (user_text or "").lower()
     route: Optional[str] = None
     vars_out: Dict[str, str] = {}
     next_action: Optional[str] = None

-    # 1) Preferir fluxo expl??cito
-    fp = str(v.get("fluxo_path", "")).strip().lower()
+    fp = str((v.get("fluxo_path") or "")).strip().lower()
     if fp in {"envio", "recebimento"}:
         route = fp
     else:
-        # 2) Heur??stica simples
-        if any(k in t for k in ["recebi", "receb", "chegou", "abriu", "abertura", "li", "confirmacao de leitura"]):
+        if any(k in t for k in ["recebi", "receb", "chegou", "abriu", "abertura", "confirmacao de leitura"]):
             route = "recebimento"
         elif any(k in t for k in ["enviar", "envio", "mandar", "disparar", "disparo", "quero enviar"]):
             route = "envio"

-    # 3) Se envio, calcular volumetria
     if route == "envio":
         limiar = int(v.get("VOLUME_ALTO_LIMIAR", 1200))
         vol_src = str(v.get("lead_volumetria", v.get("lead_duvida", ""))).lower()

-        n = None
+        n: Optional[int] = None
         m = re.findall(r"\d{1,3}(?:[\.,]\d{3})+|\d+", vol_src)
         if m:
-            token = re.sub(r"[^\d]", "", m[-1])
-            if token:
-                n = int(token)
+            digits = re.sub(r"[^\d]", "", m[-1])
+            if digits:
+                n = int(digits)

         kw_high = re.search(r"(alto volume|grande volume|massivo|lote|mil|1k|1000\+|acima de|>\s*1000)", vol_src)
         is_high = (n is not None and n >= limiar) or bool(kw_high)
@@ -111,102 +148,173 @@ def classify_route(user_text: str, v: Dict[str, Any]) -> Tuple[Optional[str], Di

     return route, vars_out, next_action

-# ---------------------------------------------------------------------------
-# Utilidades OpenAI (opcionais)
-# ---------------------------------------------------------------------------
+# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
+# RAG (Supabase + OpenAI) ??? endpoint interno
+# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
+SUPABASE_URL  = (os.getenv("SUPABASE_URL", "") or "").rstrip("/")
+SUPABASE_KEY  = os.getenv("SUPABASE_SERVICE_ROLE_KEY", "")
+EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "text-embedding-3-small")
+EMBEDDING_DIM   = int(os.getenv("EMBEDDING_DIM", "1536"))

-def wait_run(thread_id: str, run_id: str, timeout_seconds: Optional[float] = None):
-    if client is None:
-        return None
-    deadline: Optional[float] = None
-    if timeout_seconds is not None and timeout_seconds > 0:
-        deadline = time.time() + timeout_seconds
-    while True:
-        run = client.beta.threads.runs.retrieve(thread_id=thread_id, run_id=run_id)
-        if run.status in ("completed", "failed", "cancelled", "expired"):
-            return run
-        if run.status == "requires_action":
-            return run
-        if deadline is not None and time.time() >= deadline:
-            return None
-        time.sleep(0.6)
+HEADERS_JSON = {
+    "apikey": SUPABASE_KEY,
+    "Authorization": f"Bearer {SUPABASE_KEY}",
+    "Content-Type": "application/json",
+}

+class RagQuery(BaseModel):
+    question: str
+    k: int = 5
+    filter_source: Optional[str] = None

-def last_assistant_message(thread_id: str) -> str:
-    if client is None:
-        return ""
-    msgs = client.beta.threads.messages.list(thread_id=thread_id)
-    for msg in msgs.data:
-        if getattr(msg, "role", "") == "assistant":
-            try:
-                return msg.content[0].text.value
-            except Exception:
-                continue
-    return ""
+class RagHit(BaseModel):
+    content: str
+    metadata: Dict[str, Any] = {}
+    similarity: float
+
+class RagResponse(BaseModel):
+    hits: List[RagHit]
+    context: str
+
+def _embed(q: str) -> List[float]:
+    if OpenAI is None:
+        raise RuntimeError("SDK OpenAI n??o dispon??vel")
+    client = OpenAI(api_key=OPENAI_API_KEY)
+    vec = client.embeddings.create(model=EMBEDDING_MODEL, input=[q]).data[0].embedding
+    if len(vec) != EMBEDDING_DIM:
+        raise RuntimeError(f"Embedding dim {len(vec)} != {EMBEDDING_DIM}")
+    return vec
+
+def _rpc_match(query_emb: List[float], k: int, filter_source: Optional[str]):
+    url = f"{SUPABASE_URL}/rest/v1/rpc/match_aria_chunks"
+    payload = {"query_embedding": query_emb, "match_count": int(k), "filter_source": filter_source}
+    r = requests.post(url, headers=HEADERS_JSON, data=json.dumps(payload), timeout=30)
+    if r.status_code >= 300:
+        raise RuntimeError(f"RPC match failed: {r.status_code} -> {r.text}")
+    return r.json()

-# ---------------------------------------------------------------------------
-# Endpoint
-# ---------------------------------------------------------------------------
+@app.post("/rag/query", response_model=RagResponse)
+def rag_query(q: RagQuery):
+    vec  = _embed(q.question)
+    rows = _rpc_match(vec, q.k, q.filter_source)
+    hits = [RagHit(content=r.get("content",""), metadata=r.get("metadata") or {}, similarity=float(r.get("similarity", 0))) for r in rows]
+    context = "\n\n".join(f"[{i+1}] {h.content}" for i, h in enumerate(hits))
+    return RagResponse(hits=hits, context=context)

+# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
+# Heur??stica para acionar RAG + cliente interno
+# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
+KEYWORDS = ("como", "funciona", "pre??o", "prazo", "o que ??", "qual", "como fa??o")
+
+def want_rag(text: str, v: Dict[str, Any]) -> bool:
+    if (v or {}).get("faq_mode") is True:
+        return True
+    t = (text or "").lower()
+    return any(k in t for k in KEYWORDS)
+
+RAG_ENABLE          = os.getenv("RAG_ENABLE", "true").lower() == "true"
+RAG_ENDPOINT        = os.getenv("RAG_ENDPOINT", "http://127.0.0.1:8000/rag/query")
+RAG_DEFAULT_SOURCE  = os.getenv("RAG_DEFAULT_SOURCE", "faq")
+
+def fetch_rag_context(question: str, k: int = 5, filter_source: Optional[str] = RAG_DEFAULT_SOURCE, timeout: int = 12) -> Optional[str]:
+    payload = {"question": question, "k": int(k), "filter_source": filter_source}
+    start = time.time()
+    try:
+        session = get_rag_session()
+        r = session.post(RAG_ENDPOINT, json=payload, timeout=timeout)
+        r.raise_for_status()
+        data = r.json() or {}
+        ctx = data.get("context") or None
+        log.debug(
+            "RAG context ok in %.2fs (k=%s, source=%s, size=%s)",
+            time.time() - start,
+            k,
+            filter_source,
+            len(ctx or ""),
+        )
+        return ctx
+    except requests.Timeout:
+        log.warning("RAG timeout after %ss", timeout)
+        return None
+    except Exception as e:
+        log.warning("RAG offline/erro: %s", e)
+        return None
+
+# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
+# Endpoint principal
+# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
 @app.post("/assist/routing", response_model=AssistResponse)
 def assist_routing(req: AssistRequest, _tok: str = Depends(require_auth)):
-    try:
-        user_text = (req.input or req.user_text or "").strip()
-        v_in: Dict[str, Any] = dict(req.variables or {})
+    user_text = (req.input or req.user_text or "").strip()
+    v_in: Dict[str, Any] = dict(req.variables or {})

-        # Regras determin??sticas de neg??cio
-        route, vars_out, next_action = classify_route(user_text, v_in)
+    # 1) Regras determin??sticas
+    route, vars_out, next_action = classify_route(user_text, v_in)

-        # Thread
-        if req.thread_id:
-            thread_id = req.thread_id
-        else:
-            thread_id = f"thread_{secrets.token_hex(8)}"
-            if client is not None:
-                thread_id = client.beta.threads.create().id
+    # 2) RAG opcional
+    rag_ctx: Optional[str] = None
+    need_rag = RAG_ENABLE and want_rag(user_text, v_in)
+    if need_rag:
+        rag_ctx = fetch_rag_context(user_text)
+        if rag_ctx:
+            vars_out["need_rag"] = "true"
+            vars_out["rag_context"] = rag_ctx

-        # Se houver OpenAI Assistant, registra mensagem e executa run
-        reply_text = ""
-        if client is not None and ASSISTANT_ID:
+    # 3) Thread (se usar Assistant)
+    if req.thread_id:
+        thread_id = req.thread_id
+    else:
+        thread_id = f"thread_{secrets.token_hex(8)}"
+        if client_assistant is not None:
             try:
-                client.beta.threads.messages.create(thread_id=thread_id, role="user", content=user_text)
-                run = client.beta.threads.runs.create(thread_id=thread_id, assistant_id=ASSISTANT_ID)
-                run = wait_run(thread_id, run.id, timeout_seconds=ASSISTANT_TIMEOUT_SECONDS)
-                reply_text = (last_assistant_message(thread_id) or reply_text)
+                thread_id = client_assistant.beta.threads.create().id
             except Exception:
-                # Em caso de erro/timeout no Assistant, segue com fallback determin??stico
                 pass

-        # Fallback de resposta se n??o houver assistant ou n??o retornou texto
-        if not reply_text:
-            if route == "recebimento":
-                reply_text = (
-                    "Entendi que voc?? recebeu uma notifica????o. A AR Online ?? o meio de envio; "
-                    "o conte??do deve ser tratado diretamente com o remetente indicado na mensagem."
-                )
-            elif route == "envio":
-                reply_text = (
-                    "Certo! Para indicar o melhor caminho, me informe uma estimativa do volume mensal de envios (ex.: 50, 300, 1500)."
-                )
-            else:
-                reply_text = "Como posso te ajudar hoje?"
-
-        return AssistResponse(
-            reply_text=reply_text,
-            route=route,
-            variables=vars_out or None,
-            confidence=0.75 if route else None,
-            next_action=next_action,
-            tags=[],
-            thread_id=thread_id,
-        )
+    # 4) Assistant opcional
+    reply_text = ""
+    if client_assistant is not None and ASSISTANT_ID:
+        try:
+            system_rules = (
+                "Voc?? ?? a ARIA. Responda em pt-BR. "
+                "Use APENAS o CONTEXTO quando fornecido; se faltar, diga que n??o encontrou e ofere??a encaminhar ao time."
+            )
+            prompt = system_rules + (f"\n\nCONTEXTO:\n{rag_ctx}\n\n" if rag_ctx else "\n\n") + f"PERGUNTA:\n{user_text}"
+            client_assistant.beta.threads.messages.create(thread_id=thread_id, role="user", content=prompt)
+            run = client_assistant.beta.threads.runs.create(thread_id=thread_id, assistant_id=ASSISTANT_ID)
+            wait_run(thread_id, run.id, ASSISTANT_TIMEOUT_SECONDS)
+            reply_text = last_assistant_message(thread_id)
+        except Exception:
+            reply_text = ""

-    except HTTPException:
-        raise
-    except Exception as e:
-        raise HTTPException(status_code=500, detail=str(e))
+    # 5) Fallback determin??stico
+    if not reply_text:
+        if rag_ctx:
+            reply_text = "Encontrei estes trechos relevantes e respondi com base neles."
+            route = route or "faq"
+        elif route == "recebimento":
+            reply_text = (
+                "Entendi que voc?? recebeu uma notifica????o. A AR Online ?? o meio de envio; "
+                "o conte??do deve ser tratado diretamente com o remetente indicado na mensagem."
+            )
+        elif route == "envio":
+            reply_text = "Certo! Informe uma estimativa do volume mensal (ex.: 50, 300, 1500) para sugerir o melhor caminho."
+        else:
+            reply_text = "Como posso te ajudar hoje?"

+    return AssistResponse(
+        reply_text=reply_text,
+        route=route,
+        thread_id=thread_id,
+        variables=vars_out or None,
+        confidence=0.75 if route else None,
+        next_action=next_action,
+        tags=[],
+    )

+# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
+# Health
+# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
 @app.get("/healthz")
 def healthz():
     return {"ok": True}
